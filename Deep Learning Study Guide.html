<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>22CSE157 - Fundamentals of Deep Learning Complete Solutions</title>
    <style>
        :root {
            --primary-color: #2d5f7f;
            --secondary-color: #4a90a4;
            --accent-color: #f39c12;
            --success-color: #27ae60;
            --danger-color: #e74c3c;
            --light-bg: #ecf0f1;
            --dark-text: #2c3e50;
            --border-color: #bdc3c7;
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            line-height: 1.6;
            color: var(--dark-text);
            background: linear-gradient(135deg, #f5f7fa 0%, #c3cfe2 100%);
            min-height: 100vh;
        }

        .container {
            max-width: 1400px;
            margin: 0 auto;
            padding: 20px;
        }

        header {
            background: linear-gradient(135deg, var(--primary-color) 0%, var(--secondary-color) 100%);
            color: white;
            padding: 40px 20px;
            text-align: center;
            border-radius: 10px;
            margin-bottom: 30px;
            box-shadow: 0 4px 6px rgba(0,0,0,0.1);
        }

        header h1 {
            font-size: 2.5em;
            margin-bottom: 10px;
        }

        header p {
            font-size: 1.1em;
            opacity: 0.9;
        }

        .nav-tabs {
            display: flex;
            gap: 10px;
            margin-bottom: 20px;
            flex-wrap: wrap;
            background: white;
            padding: 15px;
            border-radius: 8px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }

        .nav-tabs button {
            padding: 10px 20px;
            border: 2px solid var(--border-color);
            background: white;
            color: var(--dark-text);
            border-radius: 5px;
            cursor: pointer;
            font-weight: 600;
            transition: all 0.3s ease;
            font-size: 0.95em;
        }

        .nav-tabs button:hover {
            background: var(--light-bg);
            border-color: var(--secondary-color);
        }

        .nav-tabs button.active {
            background: var(--primary-color);
            color: white;
            border-color: var(--primary-color);
        }

        .content {
            background: white;
            padding: 30px;
            border-radius: 10px;
            box-shadow: 0 4px 8px rgba(0,0,0,0.1);
            margin-bottom: 20px;
        }

        .section {
            display: none;
        }

        .section.active {
            display: block;
        }

        .section h2 {
            color: var(--primary-color);
            border-bottom: 3px solid var(--accent-color);
            padding-bottom: 10px;
            margin-bottom: 20px;
            font-size: 1.8em;
        }

        .section h3 {
            color: var(--secondary-color);
            margin-top: 25px;
            margin-bottom: 15px;
            font-size: 1.3em;
        }

        .question-block {
            background: var(--light-bg);
            padding: 20px;
            border-left: 5px solid var(--accent-color);
            margin-bottom: 20px;
            border-radius: 5px;
        }

        .question-title {
            font-weight: 700;
            color: var(--primary-color);
            font-size: 1.1em;
            margin-bottom: 10px;
        }

        .marks {
            display: inline-block;
            background: var(--accent-color);
            color: white;
            padding: 3px 10px;
            border-radius: 20px;
            font-size: 0.85em;
            font-weight: 600;
            margin-left: 10px;
        }

        .answer {
            color: var(--dark-text);
            line-height: 1.8;
            margin-top: 10px;
        }

        .answer p {
            margin-bottom: 12px;
        }

        .answer ul, .answer ol {
            margin-left: 20px;
            margin-bottom: 12px;
        }

        .answer li {
            margin-bottom: 8px;
        }

        .emphasis {
            background: #fff3cd;
            padding: 15px;
            border-left: 4px solid #ffc107;
            margin: 15px 0;
            border-radius: 3px;
        }

        .example-box {
            background: #e8f5e9;
            padding: 15px;
            border-left: 4px solid var(--success-color);
            margin: 15px 0;
            border-radius: 3px;
        }

        .formula-box {
            background: #f3e5f5;
            padding: 15px;
            border-left: 4px solid #9c27b0;
            margin: 15px 0;
            border-radius: 3px;
            font-family: 'Courier New', monospace;
            overflow-x: auto;
        }

        .image-container {
            text-align: center;
            margin: 20px 0;
            padding: 15px;
            background: #f9f9f9;
            border-radius: 5px;
        }

        .image-container img {
            max-width: 100%;
            height: auto;
            border-radius: 5px;
            box-shadow: 0 2px 8px rgba(0,0,0,0.15);
        }

        .image-caption {
            font-size: 0.9em;
            color: var(--secondary-color);
            margin-top: 10px;
            font-weight: 600;
        }

        .part-header {
            background: linear-gradient(90deg, var(--primary-color) 0%, var(--secondary-color) 100%);
            color: white;
            padding: 15px 20px;
            border-radius: 5px;
            margin: 25px 0 20px 0;
            font-size: 1.3em;
            font-weight: 700;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin: 15px 0;
            background: white;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
            border-radius: 5px;
            overflow: hidden;
        }

        table thead {
            background: var(--primary-color);
            color: white;
        }

        table th {
            padding: 12px;
            text-align: left;
            font-weight: 700;
        }

        table td {
            padding: 12px;
            border-bottom: 1px solid var(--border-color);
        }

        table tbody tr:hover {
            background: var(--light-bg);
        }

        .summary-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
            gap: 20px;
            margin: 20px 0;
        }

        .summary-card {
            background: white;
            border: 2px solid var(--border-color);
            border-radius: 8px;
            padding: 20px;
            transition: all 0.3s ease;
        }

        .summary-card:hover {
            transform: translateY(-5px);
            box-shadow: 0 4px 12px rgba(0,0,0,0.15);
            border-color: var(--secondary-color);
        }

        .summary-card h4 {
            color: var(--primary-color);
            margin-bottom: 10px;
            font-size: 1.1em;
        }

        .summary-card p {
            font-size: 0.9em;
            color: #555;
        }

        .btn-group {
            display: flex;
            gap: 10px;
            margin: 20px 0;
            flex-wrap: wrap;
        }

        .btn {
            padding: 10px 20px;
            border: none;
            border-radius: 5px;
            cursor: pointer;
            font-weight: 600;
            transition: all 0.3s ease;
            font-size: 0.95em;
        }

        .btn-primary {
            background: var(--primary-color);
            color: white;
        }

        .btn-primary:hover {
            background: var(--secondary-color);
            transform: translateY(-2px);
            box-shadow: 0 4px 8px rgba(0,0,0,0.2);
        }

        .btn-secondary {
            background: var(--accent-color);
            color: white;
        }

        .btn-secondary:hover {
            background: #e67e22;
        }

        .footer {
            background: var(--primary-color);
            color: white;
            text-align: center;
            padding: 20px;
            border-radius: 10px;
            margin-top: 30px;
        }

        .comparison-table {
            font-size: 0.95em;
        }

        .comparison-table td:first-child {
            font-weight: 600;
            color: var(--primary-color);
        }

        .progress-bar {
            background: var(--light-bg);
            border-radius: 10px;
            height: 30px;
            margin: 10px 0;
            overflow: hidden;
        }

        .progress-fill {
            background: linear-gradient(90deg, var(--primary-color), var(--secondary-color));
            height: 100%;
            display: flex;
            align-items: center;
            justify-content: center;
            color: white;
            font-weight: 600;
            font-size: 0.85em;
        }

        @media (max-width: 768px) {
            header h1 {
                font-size: 1.8em;
            }

            .nav-tabs {
                flex-direction: column;
            }

            .nav-tabs button {
                width: 100%;
            }

            .summary-grid {
                grid-template-columns: 1fr;
            }

            .content {
                padding: 20px;
            }
        }

        .search-box {
            width: 100%;
            padding: 12px;
            margin-bottom: 20px;
            border: 2px solid var(--border-color);
            border-radius: 5px;
            font-size: 1em;
        }

        .search-box:focus {
            outline: none;
            border-color: var(--primary-color);
            box-shadow: 0 0 5px rgba(45, 95, 127, 0.3);
        }

        .no-results {
            text-align: center;
            padding: 20px;
            color: var(--danger-color);
            font-weight: 600;
        }
    </style>
</head>
<body>
    <div class="container">
        <header>
            <h1>üìö Fundamentals of Deep Learning (22CSE157)</h1>
            <p>Complete Solutions for Internal Assessments I & II with Visual Illustrations</p>
            <p>Course: CSEAI | Semester 6 | Max Marks: 50 Each</p>
        </header>

        <div class="nav-tabs">
            <button class="nav-btn active" data-section="overview">üìã Overview</button>
            <button class="nav-btn" data-section="ia1-part-a">IA1 - Part A (2M)</button>
            <button class="nav-btn" data-section="ia1-part-b">IA1 - Part B (5M)</button>
            <button class="nav-btn" data-section="ia1-part-c">IA1 - Part C (10M)</button>
            <button class="nav-btn" data-section="ia2-part-a">IA2 - Part A (2M)</button>
            <button class="nav-btn" data-section="ia2-part-b">IA2 - Part B (5M)</button>
            <button class="nav-btn" data-section="ia2-part-c">IA2 - Part C (10M)</button>
            <button class="nav-btn" data-section="summary">üìä Summary & Resources</button>
        </div>

        <input type="text" class="search-box" id="searchBox" placeholder="üîç Search questions or topics...">

        <!-- OVERVIEW SECTION -->
        <div class="content">
            <div class="section active" id="overview">
                <h2>üìñ Overview</h2>
                
                <div class="emphasis">
                    <strong>Course Code:</strong> 22CSE157<br>
                    <strong>Course Name:</strong> Fundamentals of Deep Learning<br>
                    <strong>Program:</strong> CSEAI (Semester 6)<br>
                    <strong>Max Marks:</strong> 50 (Each Assessment)<br>
                    <strong>Duration:</strong> 90 minutes
                </div>

                <h3>üìå Course Outcomes (CO)</h3>
                <ul>
                    <li><strong>CO1:</strong> Explain the basics of single layer perceptron networks</li>
                    <li><strong>CO2:</strong> Explain the basics of multilayer perceptron networks</li>
                    <li><strong>CO3:</strong> Solve real world problems using neural networks</li>
                    <li><strong>CO4:</strong> Use RNN and LSTM to solve real life problems</li>
                    <li><strong>CO5:</strong> Identify the applications of auto encoders to solve them</li>
                    <li><strong>CO6:</strong> Examine the benefits of CNN in real life applications</li>
                </ul>

                <h3>üìä Assessment Structure</h3>
                <table>
                    <thead>
                        <tr>
                            <th>Part</th>
                            <th>Questions</th>
                            <th>Marks Each</th>
                            <th>Total Marks</th>
                            <th>Bloom's Level</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>Part A</td>
                            <td>5 (Answer All)</td>
                            <td>2</td>
                            <td>10</td>
                            <td>L2 (Understand)</td>
                        </tr>
                        <tr>
                            <td>Part B</td>
                            <td>5 (Answer Any 4)</td>
                            <td>5</td>
                            <td>20</td>
                            <td>L3 (Apply)</td>
                        </tr>
                        <tr>
                            <td>Part C</td>
                            <td>3 (Answer Any 2)</td>
                            <td>10</td>
                            <td>20</td>
                            <td>L3 (Apply)</td>
                        </tr>
                    </tbody>
                </table>

                <h3>üéØ Key Concepts Covered</h3>
                <div class="summary-grid">
                    <div class="summary-card">
                        <h4>üîÑ Recurrent Neural Networks (RNN)</h4>
                        <p>Sequential data processing with temporal dependencies and hidden state memory</p>
                    </div>
                    <div class="summary-card">
                        <h4>üé® Autoencoders</h4>
                        <p>Unsupervised learning for dimensionality reduction and feature extraction</p>
                    </div>
                    <div class="summary-card">
                        <h4>üñºÔ∏è Convolutional Neural Networks (CNN)</h4>
                        <p>Image processing with convolution filters and feature hierarchies</p>
                    </div>
                    <div class="summary-card">
                        <h4>üìä Perceptrons</h4>
                        <p>Single and multi-layer networks for classification problems</p>
                    </div>
                    <div class="summary-card">
                        <h4>üîô Backpropagation</h4>
                        <p>Training algorithm for computing gradients and updating weights</p>
                    </div>
                    <div class="summary-card">
                        <h4>‚è±Ô∏è Hidden Markov Models (HMM)</h4>
                        <p>Statistical models for sequential data with hidden states</p>
                    </div>
                </div>

                <div class="image-container">
                    <img src="https://user-gen-media-assets.s3.amazonaws.com/seedream_images/ca2970d0-2d53-432d-8c00-b22ba78c6a29.png" alt="Deep Learning Concepts Summary">
                    <div class="image-caption">Complete Deep Learning Concepts Summary</div>
                </div>
            </div>

            <!-- IA1 PART A -->
            <div class="section" id="ia1-part-a">
                <h2>Internal Assessment - I (IA1) - Part A</h2>
                <p style="color: var(--secondary-color); font-weight: 600;">2 Marks Each | Answer All 5 Questions | Total: 10 Marks | Bloom's Level: L2 (Understanding)</p>

                <div class="question-block">
                    <div class="question-title">Q1a: Describe a pattern with example <span class="marks">2M | CO1</span></div>
                    <div class="answer">
                        <p><strong>Pattern Definition:</strong> A pattern is a recurring, meaningful structure or relationship in data that can be learned and recognized by a system. It represents characteristic features or configurations that appear consistently across similar instances.</p>
                        
                        <div class="emphasis">
                            <strong>Key Points:</strong>
                            <ul>
                                <li>Patterns form the basis for training neural networks</li>
                                <li>Enable the system to make predictions on new, unseen data</li>
                                <li>Can be visual, temporal, statistical, or structural</li>
                            </ul>
                        </div>

                        <div class="example-box">
                            <strong>Example - Cat Recognition:</strong><br>
                            Pattern features: pointed ears, whiskers, four legs, furry texture, small nose<br>
                            When a network learns these patterns from training images, it can recognize new images containing similar patterns as cats, even if it hasn't seen those exact images before.
                        </div>
                    </div>
                </div>

                <div class="question-block">
                    <div class="question-title">Q1b: Explain decision region and decision surface <span class="marks">2M | CO1</span></div>
                    <div class="answer">
                        <p><strong>Decision Region:</strong> A partition of the input space where all points within that region are assigned to the same class by a classifier. For a two-class problem, the input space is divided into two decision regions‚Äîone for each class.</p>

                        <p><strong>Decision Surface:</strong> The boundary that separates different decision regions. It represents the mathematical frontier where the classifier transitions from assigning inputs to one class to assigning them to another. In a linear classifier, the decision surface is a hyperplane (a line in 2D, a plane in 3D).</p>

                        <div class="example-box">
                            <strong>Example:</strong><br>
                            For spam email detection: Decision Surface = boundary separating spam emails from legitimate emails in feature space (word frequencies, sender info, etc.)
                        </div>
                    </div>
                </div>

                <div class="question-block">
                    <div class="question-title">Q1c: Describe a linearly non-separable pattern <span class="marks">2M | CO2</span></div>
                    <div class="answer">
                        <p><strong>Definition:</strong> A linearly non-separable pattern is a classification problem where no single straight line (or hyperplane in higher dimensions) can perfectly separate the two classes. The classes overlap or are intertwined in the input space.</p>

                        <div class="emphasis">
                            <strong>Characteristics:</strong>
                            <ul>
                                <li>Cannot be solved by a single-layer perceptron</li>
                                <li>Requires multi-layer neural networks with non-linear activation functions</li>
                                <li>Common in real-world scenarios with complex, overlapping distributions</li>
                            </ul>
                        </div>

                        <div class="example-box">
                            <strong>Classic Example - XOR Problem:</strong><br>
                            Input pairs: (0,0)‚Üí0, (0,1)‚Üí1, (1,0)‚Üí1, (1,1)‚Üí0<br>
                            A single line cannot separate the 0 outputs from the 1 outputs‚Äîrequires non-linear boundaries.
                        </div>
                    </div>
                </div>

                <div class="question-block">
                    <div class="question-title">Q1d: Explain linear programming problem <span class="marks">2M | CO3</span></div>
                    <div class="answer">
                        <p><strong>Definition:</strong> A linear programming (LP) problem is an optimization problem where the objective is to maximize or minimize a linear function subject to linear constraints (inequalities or equalities).</p>

                        <div class="emphasis">
                            <strong>Components:</strong>
                            <ul>
                                <li><strong>Objective function:</strong> Linear equation to optimize (maximize profit or minimize cost)</li>
                                <li><strong>Constraints:</strong> Linear inequalities defining the feasible region</li>
                                <li><strong>Decision variables:</strong> Variables that can be adjusted to achieve the optimal solution</li>
                            </ul>
                        </div>

                        <div class="example-box">
                            <strong>Application in Neural Networks:</strong><br>
                            In perceptron training and optimization, linear programming helps find weight configurations that satisfy margin constraints while minimizing classification errors.
                        </div>
                    </div>
                </div>

                <div class="question-block">
                    <div class="question-title">Q1e: Describe neurocontrol <span class="marks">2M | CO3</span></div>
                    <div class="answer">
                        <p><strong>Definition:</strong> Neurocontrol is a control strategy that uses artificial neural networks to regulate or manage dynamic systems. Unlike traditional control systems with pre-programmed rules, neurocontrol systems learn control policies through training data or interaction with the environment.</p>

                        <div class="emphasis">
                            <strong>Key Features:</strong>
                            <ul>
                                <li><strong>Adaptive:</strong> Learns and improves controller performance over time</li>
                                <li><strong>Non-linear:</strong> Can handle complex, non-linear system dynamics</li>
                                <li><strong>Real-time:</strong> Makes control decisions based on current system state</li>
                                <li><strong>Data-driven:</strong> Learns from examples rather than explicit programming</li>
                            </ul>
                        </div>

                        <div class="example-box">
                            <strong>Applications:</strong>
                            <ul>
                                <li>Robot movement control and motion planning</li>
                                <li>Industrial process control and optimization</li>
                                <li>Autonomous vehicle steering and navigation</li>
                                <li>Drone flight stabilization</li>
                            </ul>
                        </div>
                    </div>
                </div>
            </div>

            <!-- IA1 PART B -->
            <div class="section" id="ia1-part-b">
                <h2>Internal Assessment - I (IA1) - Part B</h2>
                <p style="color: var(--secondary-color); font-weight: 600;">5 Marks Each | Answer Any 4 out of 5 | Total: 20 Marks | Bloom's Level: L3 (Application)</p>

                <div class="question-block">
                    <div class="question-title">Q2a: Illustrate discriminant functions <span class="marks">5M | CO2</span></div>
                    <div class="answer">
                        <p><strong>Definition:</strong> Discriminant functions are mathematical functions used in classification that compute a score for each class. A pattern is classified into the class with the highest score.</p>

                        <div class="emphasis">
                            <strong>For Two-Class Problem:</strong>
                            <div class="formula-box">
                                g(x) = w·µÄx + w‚ÇÄ<br>
                                If g(x) > 0, assign to class œâ‚ÇÅ<br>
                                If g(x) < 0, assign to class œâ‚ÇÇ<br>
                                If g(x) = 0, the decision surface
                            </div>
                        </div>

                        <p><strong>Multi-class Discriminant Functions:</strong></p>
                        <ul>
                            <li>Compute separate scores g·µ¢(x) for each class i</li>
                            <li>Assign x to class with max(g·µ¢(x))</li>
                            <li>Decision boundaries formed where two discriminant values are equal</li>
                        </ul>

                        <div class="example-box">
                            <strong>Properties:</strong>
                            <ul>
                                <li>Linear discriminants create hyperplane decision surfaces</li>
                                <li>Non-linear discriminants create complex curved boundaries</li>
                                <li>Weights (w) determine boundary orientation</li>
                                <li>Bias (w‚ÇÄ) determines boundary position</li>
                            </ul>
                        </div>
                    </div>
                </div>

                <div class="question-block">
                    <div class="question-title">Q2b: Illustrate medical diagnosis expert system <span class="marks">5M | CO2</span></div>
                    <div class="answer">
                        <p><strong>System Purpose:</strong> A medical diagnosis expert system uses neural networks to predict diseases based on patient symptoms and medical test results.</p>

                        <div class="emphasis">
                            <strong>System Architecture:</strong>
                            <ul>
                                <li><strong>Input Layer:</strong> Patient symptoms, lab values, vital signs</li>
                                <li><strong>Hidden Layers:</strong> Extract complex patterns and relationships between symptoms</li>
                                <li><strong>Output Layer:</strong> Disease predictions (probability of each disease)</li>
                            </ul>
                        </div>

                        <p><strong>Process Flow:</strong></p>
                        <ol>
                            <li>Patient data collected (fever, cough, body temperature, etc.)</li>
                            <li>Network processes features through learned patterns</li>
                            <li>Outputs probability scores for different conditions</li>
                            <li>Highest probability diagnosis recommended to physician</li>
                        </ol>

                        <div class="example-box">
                            <strong>Real-World Example:</strong><br>
                            System trained on 10,000 patient records learns that certain symptom combinations (high fever + dry cough + loss of taste) strongly indicate COVID-19, enabling accurate diagnosis for new patients.
                        </div>
                    </div>
                </div>

                <div class="question-block">
                    <div class="question-title">Q2c: Solve XOR problem using multi-layer perceptron network <span class="marks">5M | CO2</span></div>
                    <div class="answer">
                        <p><strong>Problem Definition:</strong> The XOR (Exclusive OR) problem is a classic benchmark that cannot be solved by a single-layer perceptron but requires multiple layers.</p>

                        <div class="emphasis">
                            <strong>XOR Truth Table:</strong><br>
                            <table class="comparison-table">
                                <tr>
                                    <td>Input (x‚ÇÅ, x‚ÇÇ)</td>
                                    <td>Output (y)</td>
                                </tr>
                                <tr>
                                    <td>(0, 0)</td>
                                    <td>0</td>
                                </tr>
                                <tr>
                                    <td>(0, 1)</td>
                                    <td>1</td>
                                </tr>
                                <tr>
                                    <td>(1, 0)</td>
                                    <td>1</td>
                                </tr>
                                <tr>
                                    <td>(1, 1)</td>
                                    <td>0</td>
                                </tr>
                            </table>
                        </div>

                        <div class="image-container">
                            <img src="https://user-gen-media-assets.s3.amazonaws.com/seedream_images/288e3315-7fe4-4299-84cf-5bede30c441a.png" alt="XOR Problem Solution">
                            <div class="image-caption">XOR Problem Solution using Multi-layer Perceptron</div>
                        </div>

                        <p><strong>Multi-Layer Perceptron Solution:</strong></p>
                        <div class="emphasis">
                            <strong>Architecture:</strong>
                            <ul>
                                <li><strong>Input Layer:</strong> 2 neurons (x‚ÇÅ, x‚ÇÇ)</li>
                                <li><strong>Hidden Layer:</strong> 2-3 neurons with sigmoid activation</li>
                                <li><strong>Output Layer:</strong> 1 neuron with sigmoid activation</li>
                            </ul>
                        </div>

                        <p><strong>How It Solves XOR:</strong></p>
                        <ol>
                            <li>First hidden neuron learns to separate points where x‚ÇÅ ‚â† x‚ÇÇ</li>
                            <li>Second hidden neuron learns complementary pattern</li>
                            <li>Output layer combines these patterns to produce XOR logic</li>
                        </ol>

                        <div class="example-box">
                            <strong>Mathematical Principle:</strong><br>
                            Hidden layers create non-linear transformations that map the input space such that classes become linearly separable at the output layer. The network learns:
                            <ul>
                                <li>h‚ÇÅ = œÉ(w‚ÇÅ‚ÇÅx‚ÇÅ + w‚ÇÅ‚ÇÇx‚ÇÇ + b‚ÇÅ)</li>
                                <li>h‚ÇÇ = œÉ(w‚ÇÇ‚ÇÅx‚ÇÅ + w‚ÇÇ‚ÇÇx‚ÇÇ + b‚ÇÇ)</li>
                                <li>y = œÉ(w‚ÇÉ‚ÇÅh‚ÇÅ + w‚ÇÉ‚ÇÇh‚ÇÇ + b‚ÇÉ)</li>
                            </ul>
                        </div>
                    </div>
                </div>

                <div class="question-block">
                    <div class="question-title">Q2d: Sketch a multi-layer feedforward network for handwritten digit recognition <span class="marks">5M | CO2</span></div>
                    <div class="answer">
                        <p><strong>Application:</strong> Handwritten digit recognition (MNIST dataset) is a classic deep learning benchmark for image classification.</p>

                        <div class="emphasis">
                            <strong>Network Architecture for MNIST:</strong>
                            <ul>
                                <li><strong>Input Layer:</strong> 784 neurons (28√ó28 pixel image flattened)</li>
                                <li><strong>Hidden Layer 1:</strong> 128 neurons with ReLU activation</li>
                                <li><strong>Hidden Layer 2:</strong> 64 neurons with ReLU activation</li>
                                <li><strong>Hidden Layer 3:</strong> 32 neurons with ReLU activation</li>
                                <li><strong>Output Layer:</strong> 10 neurons with softmax activation (one per digit 0-9)</li>
                            </ul>
                        </div>

                        <p><strong>Training Process:</strong></p>
                        <ol>
                            <li>Each training image normalized to 28√ó28 pixels</li>
                            <li>Network learns features progressively (edges ‚Üí shapes ‚Üí digit patterns)</li>
                            <li>Backpropagation adjusts weights based on classification error</li>
                            <li>Softmax output produces probability distribution across 10 digits</li>
                        </ol>

                        <div class="example-box">
                            <strong>Example Output:</strong><br>
                            Input image of "3" ‚Üí Network output probabilities:<br>
                            [0.01, 0.02, 0.05, 0.89, 0.02, 0.01, 0.00, 0.00, 0.00, 0.00]<br>
                            (89% confidence it's the digit 3)
                        </div>
                    </div>
                </div>

                <div class="question-block">
                    <div class="question-title">Q2e: Illustrate forward plant identification process with a neat sketch <span class="marks">5M | CO3</span></div>
                    <div class="answer">
                        <p><strong>Definition:</strong> Forward plant identification is a control learning process where a neural network learns to model a dynamic system's behavior.</p>

                        <div class="emphasis">
                            <strong>Process Components:</strong>
                            <ul>
                                <li><strong>Input:</strong> Control signal u(t) to the plant</li>
                                <li><strong>Plant Output:</strong> Actual system response y(t)</li>
                                <li><strong>Neural Network:</strong> Learns mapping from u(t) ‚Üí y(t)</li>
                                <li><strong>Learning:</strong> Updates weights when network output differs from actual plant</li>
                            </ul>
                        </div>

                        <p><strong>Learning Steps:</strong></p>
                        <ol>
                            <li>Apply known control signals to the plant</li>
                            <li>Observe the resulting system responses</li>
                            <li>Train the network to predict these responses</li>
                            <li>Validate on new control inputs</li>
                        </ol>

                        <div class="example-box">
                            <strong>Real-World Applications:</strong>
                            <ul>
                                <li><strong>Robot Learning:</strong> Learning how robot arm responds to motor commands</li>
                                <li><strong>Process Control:</strong> Modeling chemical reactor behavior</li>
                                <li><strong>Autonomous Systems:</strong> Learning vehicle dynamics</li>
                                <li><strong>Advantage:</strong> Enable future control without direct plant knowledge</li>
                            </ul>
                        </div>
                    </div>
                </div>
            </div>

            <!-- IA1 PART C -->
            <div class="section" id="ia1-part-c">
                <h2>Internal Assessment - I (IA1) - Part C</h2>
                <p style="color: var(--secondary-color); font-weight: 600;">10 Marks Each | Answer Any 2 out of 3 | Total: 20 Marks | Bloom's Level: L3 (Application/Analysis)</p>

                <div class="question-block">
                    <div class="question-title">Q3a: Demonstrate multi-category single layer discrete perceptron training algorithm <span class="marks">10M | CO1</span></div>
                    <div class="answer">
                        <p><strong>Algorithm Overview:</strong> The multi-category single layer discrete perceptron extends binary classification to multiple classes using separate perceptrons or a single network with multiple output neurons.</p>

                        <div class="emphasis">
                            <strong>Training Algorithm Steps:</strong>
                            <ol>
                                <li><strong>Initialization:</strong>
                                    <ul>
                                        <li>Weight vectors w·µ¢ for each class i (initialized randomly or to zero)</li>
                                        <li>Learning rate Œ∑ (typically 0.1)</li>
                                        <li>Maximum iterations</li>
                                    </ul>
                                </li>
                                <li><strong>For each training epoch:</strong>
                                    <div class="formula-box">
                                        For each training sample (x, target_class):<br>
                                        &nbsp;&nbsp;Compute outputs: y·µ¢ = sign(w·µ¢·µÄx + w·µ¢‚ÇÄ) for all classes<br>
                                        &nbsp;&nbsp;If incorrect classification:<br>
                                        &nbsp;&nbsp;&nbsp;&nbsp;w_target ‚Üê w_target + Œ∑ √ó x  (Strengthen correct class)<br>
                                        &nbsp;&nbsp;&nbsp;&nbsp;w_wrong ‚Üê w_wrong - Œ∑ √ó x    (Weaken wrong class)
                                    </div>
                                </li>
                                <li><strong>Convergence:</strong>
                                    <ul>
                                        <li>Repeat until all samples correctly classified or max iterations reached</li>
                                        <li>Each sample contributes to refining class boundaries</li>
                                    </ul>
                                </li>
                            </ol>
                        </div>

                        <div class="example-box">
                            <strong>Example (3-Class Problem):</strong>
                            <ul>
                                <li><strong>Class 1 (Cat):</strong> Features like small size, pointed ears</li>
                                <li><strong>Class 2 (Dog):</strong> Features like medium size, floppy ears</li>
                                <li><strong>Class 3 (Bird):</strong> Features like small size, wings</li>
                            </ul>
                            The network learns separate decision boundaries between each pair of classes.
                        </div>
                    </div>
                </div>

                <div class="question-block">
                    <div class="question-title">Q3b: Illustrate error back propagation training algorithm <span class="marks">10M | CO2</span></div>
                    <div class="answer">
                        <p><strong>Definition:</strong> Backpropagation is the fundamental learning mechanism for multi-layer neural networks that computes gradients efficiently using the chain rule.</p>

                        <div class="image-container">
                            <img src="https://user-gen-media-assets.s3.amazonaws.com/seedream_images/346c170e-59b0-4ebe-a5f6-a052b1d9e649.png" alt="Backpropagation Algorithm">
                            <div class="image-caption">Error Backpropagation Training Algorithm Flow</div>
                        </div>

                        <div class="emphasis">
                            <strong>Key Concept:</strong><br>
                            Errors are computed at the output and then "propagated backward" through the network to adjust all weights proportionally to their contribution to the error.
                        </div>

                        <p><strong>Algorithm Steps:</strong></p>

                        <p><strong>1. Forward Pass:</strong></p>
                        <div class="formula-box">
                            Input signal propagates through network:<br>
                            z^(l) = W^(l) √ó a^(l-1) + b^(l)<br>
                            Activation: a^(l) = œÉ(z^(l))<br>
                            Produces final output ≈∑
                        </div>

                        <p><strong>2. Compute Output Error:</strong></p>
                        <div class="formula-box">
                            Loss function: L = ¬Ω(y - ≈∑)¬≤<br>
                            Output layer error: Œ¥^(L) = (≈∑ - y) ‚äô œÉ'(z^(L))<br>
                            (‚äô = element-wise multiplication)
                        </div>

                        <p><strong>3. Backward Pass:</strong></p>
                        <div class="formula-box">
                            Propagate error backward through layers:<br>
                            Œ¥^(l) = (W^(l+1)·µÄ √ó Œ¥^(l+1)) ‚äô œÉ'(z^(l))
                        </div>

                        <p><strong>4. Weight Updates:</strong></p>
                        <div class="formula-box">
                            Gradient: ‚àÇL/‚àÇW^(l) = Œ¥^(l) √ó a^(l-1)·µÄ<br>
                            Update: W^(l) ‚Üê W^(l) - Œ∑ √ó ‚àÇL/‚àÇW^(l)
                        </div>

                        <p><strong>5. Repeat for Multiple Epochs</strong></p>

                        <div class="example-box">
                            <strong>Why Backpropagation Works:</strong>
                            <ul>
                                <li><strong>Chain Rule:</strong> Computes gradient efficiently</li>
                                <li><strong>Reuses Computations:</strong> Dynamic programming principle</li>
                                <li><strong>Gradient Descent:</strong> Adjusts weights in direction of steepest error decrease</li>
                                <li><strong>Scalability:</strong> Works for networks with many layers</li>
                            </ul>
                        </div>
                    </div>
                </div>

                <div class="question-block">
                    <div class="question-title">Q3c: Illustrate feedforward control with plant inverse learning with a neat sketch <span class="marks">10M | CO3</span></div>
                    <div class="answer">
                        <p><strong>Definition:</strong> Feedforward control with plant inverse learning is a neurocontrol approach where a neural network learns the inverse mapping of a system's dynamics.</p>

                        <div class="emphasis">
                            <strong>Core Concept:</strong>
                            <ul>
                                <li><strong>Forward Model:</strong> System response y = Plant(u)</li>
                                <li><strong>Inverse Model:</strong> Control signal u = Plant‚Åª¬π(desired_y)</li>
                            </ul>
                        </div>

                        <p><strong>Learning Process:</strong></p>
                        <ol>
                            <li><strong>Data Collection Phase:</strong>
                                <ul>
                                    <li>Apply random control signals to the plant</li>
                                    <li>Record resulting outputs</li>
                                    <li>Create training pairs: (plant_output, control_signal)</li>
                                </ul>
                            </li>
                            <li><strong>Network Training:</strong>
                                <ul>
                                    <li>Input: Desired system output (trajectory)</li>
                                    <li>Output: Required control signal</li>
                                    <li>Learn inverse dynamics through supervised learning</li>
                                </ul>
                            </li>
                            <li><strong>Control Implementation:</strong>
                                <ul>
                                    <li>To achieve desired trajectory, use learned inverse model</li>
                                    <li>Network predicts required control without traditional feedback</li>
                                </ul>
                            </li>
                        </ol>

                        <div class="example-box">
                            <strong>Advantages of Feedforward Control:</strong>
                            <ul>
                                <li><strong>Speed:</strong> No feedback delay (feedforward is faster than feedback)</li>
                                <li><strong>Efficiency:</strong> Direct computation of required control</li>
                                <li><strong>Adaptability:</strong> Learns from plant behavior automatically</li>
                                <li><strong>Robustness:</strong> Can compensate for system variations</li>
                            </ul>
                        </div>

                        <div class="example-box">
                            <strong>Practical Example - Robot Arm Learning:</strong>
                            <ol>
                                <li>Record: motor commands ‚Üí arm positions</li>
                                <li>Train inverse model: desired position ‚Üí required motor commands</li>
                                <li>Control: specify desired motion ‚Üí network computes motor signals</li>
                                <li>Result: Robot learns to reach target positions accurately</li>
                            </ol>
                        </div>
                    </div>
                </div>
            </div>

            <!-- IA2 PART A -->
            <div class="section" id="ia2-part-a">
                <h2>Internal Assessment - II (IA2) - Part A</h2>
                <p style="color: var(--secondary-color); font-weight: 600;">2 Marks Each | Answer All 5 Questions | Total: 10 Marks | Bloom's Level: L2 (Understanding)</p>

                <div class="question-block">
                    <div class="question-title">Q1a: Describe briefly modelling sequences <span class="marks">2M | CO4</span></div>
                    <div class="answer">
                        <p><strong>Definition:</strong> Sequence modelling is the process of learning patterns and dependencies in sequential data where elements depend on previous elements. It involves building models that can predict future values based on past observations.</p>

                        <div class="emphasis">
                            <strong>Key Characteristics:</strong>
                            <ul>
                                <li><strong>Temporal Dependency:</strong> Current value depends on historical values</li>
                                <li><strong>Variable Length:</strong> Sequences can have different lengths</li>
                                <li><strong>Context Importance:</strong> Models must retain information from earlier steps</li>
                                <li><strong>Order Matters:</strong> Sequence order is crucial for predictions</li>
                            </ul>
                        </div>

                        <div class="example-box">
                            <strong>Applications:</strong>
                            <ul>
                                <li>Language modeling (predicting next word in text)</li>
                                <li>Time series forecasting (stock prices, weather, sales)</li>
                                <li>Music generation (predicting next musical note)</li>
                                <li>Speech recognition (understanding context in speech)</li>
                                <li>Machine translation (sequence to sequence mapping)</li>
                            </ul>
                        </div>
                    </div>
                </div>

                <div class="question-block">
                    <div class="question-title">Q1b: Explain briefly autoencoder <span class="marks">2M | CO5</span></div>
                    <div class="answer">
                        <p><strong>Definition:</strong> An autoencoder is an unsupervised neural network that learns efficient data representations by compressing input into a latent space and reconstructing it.</p>

                        <div class="emphasis">
                            <strong>Architecture:</strong>
                            <ul>
                                <li><strong>Encoder:</strong> Compresses input ‚Üí latent representation (bottleneck)</li>
                                <li><strong>Bottleneck:</strong> Lower-dimensional compressed representation</li>
                                <li><strong>Decoder:</strong> Reconstructs output from latent representation</li>
                                <li><strong>Objective:</strong> Minimize reconstruction error</li>
                            </ul>
                        </div>

                        <div class="example-box">
                            <strong>Main Purposes:</strong>
                            <ul>
                                <li>Dimensionality reduction (reducing data dimensions)</li>
                                <li>Feature extraction (learning meaningful representations)</li>
                                <li>Denoising (removing noise from data)</li>
                                <li>Anomaly detection (identifying unusual patterns)</li>
                                <li>Data compression (efficient storage)</li>
                            </ul>
                        </div>
                    </div>
                </div>

                <div class="question-block">
                    <div class="question-title">Q1c: Summarize the depth of an image <span class="marks">2M | CO6</span></div>
                    <div class="answer">
                        <p><strong>Definition:</strong> Image depth refers to the number of color channels in an image, representing different color or intensity information.</p>

                        <div class="emphasis">
                            <strong>Image Depth Types:</strong>
                            <ul>
                                <li><strong>Grayscale Images:</strong> Depth = 1 (single intensity channel, 0-255)</li>
                                <li><strong>RGB Images:</strong> Depth = 3 (red, green, blue channels)</li>
                                <li><strong>RGBA Images:</strong> Depth = 4 (includes alpha/transparency channel)</li>
                                <li><strong>Multispectral Images:</strong> Depth > 4 (infrared, thermal, satellite imagery)</li>
                            </ul>
                        </div>

                        <div class="example-box">
                            <strong>Implications:</strong>
                            <ul>
                                <li>Higher depth provides richer information about images</li>
                                <li>Increases computational requirements for processing</li>
                                <li>Requires more storage and memory</li>
                                <li>Different CNN architectures needed for different depths</li>
                            </ul>
                        </div>
                    </div>
                </div>

                <div class="question-block">
                    <div class="question-title">Q1d: Discuss briefly autoregressive models <span class="marks">2M | CO4</span></div>
                    <div class="answer">
                        <p><strong>Definition:</strong> Autoregressive models predict future values based on previous values in a sequence. The current output depends on past outputs.</p>

                        <div class="formula-box">
                            General Formula:<br>
                            y_t = f(y_{t-1}, y_{t-2}, ..., y_{t-p}) + noise<br>
                            where p is the number of previous steps considered
                        </div>

                        <div class="emphasis">
                            <strong>Key Characteristics:</strong>
                            <ul>
                                <li><strong>Self-Regressive:</strong> Uses its own previous outputs as inputs</li>
                                <li><strong>Sequential Generation:</strong> Generates output one step at a time</li>
                                <li><strong>Probabilistic:</strong> Models probability distributions over sequences</li>
                                <li><strong>Feedback Loop:</strong> Output becomes input for next step</li>
                            </ul>
                        </div>

                        <div class="example-box">
                            <strong>Examples & Applications:</strong>
                            <ul>
                                <li><strong>ARIMA Models:</strong> Classical time series forecasting</li>
                                <li><strong>RNNs/LSTMs:</strong> Deep learning sequence models</li>
                                <li><strong>GPT Models:</strong> Large language models for text generation</li>
                                <li><strong>Image Generation:</strong> PixelCNN and similar models</li>
                            </ul>
                        </div>
                    </div>
                </div>

                <div class="question-block">
                    <div class="question-title">Q1e: Describe briefly regularized autoencoders <span class="marks">2M | CO5</span></div>
                    <div class="answer">
                        <p><strong>Definition:</strong> Regularized autoencoders add penalty terms during training to prevent overfitting and force meaningful representations.</p>

                        <div class="emphasis">
                            <strong>Types of Regularization:</strong>
                            <ul>
                                <li><strong>L1/L2 Regularization:</strong> Penalizes large weights to prevent overfitting</li>
                                <li><strong>Sparse Autoencoders:</strong> Force most hidden units to be mostly inactive</li>
                                <li><strong>Variational Autoencoders (VAE):</strong> Add probabilistic constraints (KL divergence)</li>
                                <li><strong>Dropout:</strong> Randomly deactivate neurons during training</li>
                                <li><strong>Batch Normalization:</strong> Stabilize training and improve generalization</li>
                            </ul>
                        </div>

                        <div class="example-box">
                            <strong>Benefits of Regularization:</strong>
                            <ul>
                                <li>More robust feature representations</li>
                                <li>Better generalization to unseen data</li>
                                <li>Prevents learning trivial identity mappings</li>
                                <li>Creates interpretable latent spaces</li>
                                <li>Improved stability during training</li>
                            </ul>
                        </div>
                    </div>
                </div>
            </div>

            <!-- IA2 PART B -->
            <div class="section" id="ia2-part-b">
                <h2>Internal Assessment - II (IA2) - Part B</h2>
                <p style="color: var(--secondary-color); font-weight: 600;">5 Marks Each | Answer Any 4 out of 5 | Total: 20 Marks | Bloom's Level: L3 (Application/Analysis)</p>

                <div class="question-block">
                    <div class="question-title">Q2a: Illustrate linear dynamical systems <span class="marks">5M | CO4</span></div>
                    <div class="answer">
                        <p><strong>Definition:</strong> Linear dynamical systems (LDS) model how system state evolves over time according to linear transformations. They capture temporal dependencies in sequential data.</p>

                        <div class="image-container">
                            <img src="https://user-gen-media-assets.s3.amazonaws.com/seedream_images/3f46d4be-c59a-47c7-a668-036bdb6421da.png" alt="Linear Dynamical Systems">
                            <div class="image-caption">Linear Dynamical Systems for Sequence Modeling</div>
                        </div>

                        <div class="emphasis">
                            <strong>Mathematical Framework:</strong><br>
                            <div class="formula-box">
                                State transition: x_{t+1} = A √ó x_t + B √ó u_t<br>
                                Observation: y_t = C √ó x_t + D √ó u_t
                            </div>
                        </div>

                        <p><strong>Where:</strong></p>
                        <ul>
                            <li><strong>x_t:</strong> Hidden state at time t (internal system state)</li>
                            <li><strong>u_t:</strong> Input signal at time t (control input)</li>
                            <li><strong>y_t:</strong> Observed output at time t (measurement)</li>
                            <li><strong>A:</strong> State transition matrix (how state evolves)</li>
                            <li><strong>B:</strong> Input matrix (how input affects state)</li>
                            <li><strong>C:</strong> Observation matrix (how state produces output)</li>
                            <li><strong>D:</strong> Direct feedthrough matrix (direct input-output coupling)</li>
                        </ul>

                        <div class="example-box">
                            <strong>Key Characteristics:</strong>
                            <ul>
                                <li>Captures temporal dependencies in sequences</li>
                                <li>Deterministic evolution (in absence of noise)</li>
                                <li>Used for sequence modeling and prediction</li>
                                <li>Foundation for Hidden Markov Models (HMM)</li>
                                <li>Applicable to many physical and biological systems</li>
                            </ul>
                        </div>
                    </div>
                </div>

                <div class="question-block">
                    <div class="question-title">Q2b: Interpret the training process of autoencoder <span class="marks">5M | CO5</span></div>
                    <div class="answer">
                        <p><strong>Overview:</strong> Autoencoder training is an unsupervised learning process where the network learns to compress and reconstruct data.</p>

                        <div class="image-container">
                            <img src="https://user-gen-media-assets.s3.amazonaws.com/seedream_images/203ebcdb-174b-4ae0-96f3-424605cf6de1.png" alt="Autoencoder Architecture and Training">
                            <div class="image-caption">Autoencoder Network Architecture and Training Process</div>
                        </div>

                        <div class="part-header">Training Algorithm Steps</div>

                        <p><strong>1. Initialization:</strong></p>
                        <ul>
                            <li>Initialize encoder and decoder weights randomly</li>
                            <li>Choose optimization algorithm (Adam, SGD, etc.)</li>
                            <li>Define loss function (Mean Squared Error, Cross-Entropy, etc.)</li>
                        </ul>

                        <p><strong>2. Forward Pass:</strong></p>
                        <ul>
                            <li>Input x passes through encoder</li>
                            <li>Produces latent representation z = Encoder(x)</li>
                            <li>Decoder reconstructs: xÃÇ = Decoder(z)</li>
                        </ul>

                        <p><strong>3. Loss Computation:</strong></p>
                        <div class="formula-box">
                            Reconstruction loss: L = ||x - xÃÇ||¬≤<br>
                            This measures how well the network reconstructs the original
                        </div>

                        <p><strong>4. Backpropagation:</strong></p>
                        <ul>
                            <li>Gradients computed with respect to all weights</li>
                            <li>Both encoder and decoder weights updated</li>
                            <li>Error signals flow backward through both components</li>
                        </ul>

                        <p><strong>5. Iteration:</strong></p>
                        <ul>
                            <li>Repeat for all training samples</li>
                            <li>Multiple epochs until convergence</li>
                            <li>Monitor validation loss for early stopping</li>
                        </ul>

                        <div class="example-box">
                            <strong>Key Learning Aspects:</strong>
                            <ul>
                                <li><strong>Encoder learns:</strong> Most informative features that preserve essential information</li>
                                <li><strong>Decoder learns:</strong> How to reconstruct original data from compressed representation</li>
                                <li><strong>Bottleneck forces:</strong> Information compression and dimensionality reduction</li>
                                <li><strong>Training dynamics:</strong> Early epochs show large errors; over time, error decreases</li>
                            </ul>
                        </div>

                        <p><strong>Why Reconstruction Works:</strong></p>
                        <ul>
                            <li>Network cannot simply copy input due to bottleneck constraint</li>
                            <li>Must learn essential features for reconstruction</li>
                            <li>Irrelevant information naturally discarded</li>
                            <li>Results in more robust, meaningful representations</li>
                        </ul>
                    </div>
                </div>

                <div class="question-block">
                    <div class="question-title">Q2c: Interpret the image seen by a computer <span class="marks">5M | CO6</span></div>
                    <div class="answer">
                        <p><strong>Fundamental Difference:</strong> Computers perceive images as numerical matrices, not semantic concepts. Understanding this is crucial for deep learning.</p>

                        <div class="part-header">How Computers Perceive Images</div>

                        <p><strong>1. Pixel Representation:</strong></p>
                        <div class="formula-box">
                            Image = Matrix of numbers (pixel intensities)<br>
                            <br>
                            Grayscale: Values 0-255 (black to white)<br>
                            Color (RGB): Three matrices (red, green, blue channels)<br>
                            Each pixel: [R, G, B] tuple of values
                        </div>

                        <p><strong>2. Mathematical Perspective:</strong></p>
                        <div class="example-box">
                            Computer sees this image as numerical data:<br>
                            <div class="formula-box">
                                [[212, 145, 67],<br>
                                &nbsp;[189, 234, 111],<br>
                                &nbsp;[156, 198, 223]]
                            </div>
                            Not as "red apple" but as a matrix of numerical values
                        </div>

                        <p><strong>3. Feature Extraction Process (CNN Hierarchy):</strong></p>
                        <ul>
                            <li><strong>Low-level (Layer 1-2):</strong> Detects edges, corners, simple textures</li>
                            <li><strong>Mid-level (Layer 3-4):</strong> Recognizes shapes, patterns, complex textures</li>
                            <li><strong>High-level (Layer 5+):</strong> Identifies objects, faces, scenes, semantic concepts</li>
                        </ul>

                        <p><strong>4. CNN Processing Pipeline:</strong></p>
                        <ol>
                            <li>Convolution filters scan image for patterns</li>
                            <li>Each filter activates for specific learned patterns</li>
                            <li>Pooling reduces spatial dimensions</li>
                            <li>Deeper layers combine patterns into concepts</li>
                            <li>Final layers produce classification probabilities</li>
                        </ol>

                        <div class="example-box">
                            <strong>Semantic Understanding Process:</strong><br>
                            Network map: Pixel values ‚Üí Feature activations ‚Üí Learned representations ‚Üí Classification<br>
                            <br>
                            Example: Cat image processing<br>
                            Pixel matrix ‚Üí Edge detection ‚Üí Ear shapes ‚Üí Face features ‚Üí "Cat" classification
                        </div>

                        <p><strong>5. Difference from Human Vision:</strong></p>
                        <table class="comparison-table">
                            <tr>
                                <td><strong>Aspect</strong></td>
                                <td><strong>Human Vision</strong></td>
                                <td><strong>Computer Vision</strong></td>
                            </tr>
                            <tr>
                                <td>Perception</td>
                                <td>Holistic, contextual</td>
                                <td>Mathematical transformations</td>
                            </tr>
                            <tr>
                                <td>Understanding</td>
                                <td>Semantic concepts</td>
                                <td>Statistical patterns</td>
                            </tr>
                            <tr>
                                <td>Speed</td>
                                <td>Very fast, parallel</td>
                                <td>Slower, sequential layers</td>
                            </tr>
                            <tr>
                                <td>Learning</td>
                                <td>Biological experience</td>
                                <td>Training data and backpropagation</td>
                            </tr>
                        </table>
                    </div>
                </div>

                <div class="question-block">
                    <div class="question-title">Q2d: Illustrate the fundamental limitations of HMM <span class="marks">5M | CO4</span></div>
                    <div class="answer">
                        <p><strong>Context:</strong> Hidden Markov Models were among the first probabilistic models for sequence data, but have several significant limitations that led to development of RNNs and modern deep learning approaches.</p>

                        <div class="part-header">Fundamental Limitations of HMM</div>

                        <p><strong>1. Markov Assumption Limitation:</strong></p>
                        <ul>
                            <li><strong>Assumption:</strong> Only the immediate previous state matters</li>
                            <li><strong>Problem:</strong> Cannot capture long-range dependencies</li>
                            <li><strong>Example:</strong> In language, word meaning depends on entire sentence context, not just previous word</li>
                            <li><strong>Mathematical:</strong> P(state_t | state_{t-1}) ignores state_{t-2}, state_{t-3}, etc.</li>
                        </ul>

                        <p><strong>2. Limited Temporal Context:</strong></p>
                        <ul>
                            <li>Cannot effectively model phenomena with long-term memory</li>
                            <li>Information from distant past is ignored</li>
                            <li>Example: Understanding a conversation requires remembering context from minutes ago</li>
                            <li>Example: Speech processing needs broader phonetic context</li>
                        </ul>

                        <p><strong>3. Discrete Hidden States:</strong></p>
                        <ul>
                            <li>Assumes discrete, finite hidden states</li>
                            <li>Cannot represent continuous latent spaces</li>
                            <li>Limits representational capacity for complex phenomena</li>
                            <li>Real-world systems often have continuous state spaces</li>
                        </ul>

                        <p><strong>4. Independence Assumption:</strong></p>
                        <ul>
                            <li>Assumes features are conditionally independent given hidden state</li>
                            <li>Often violated in real data with complex feature correlations</li>
                            <li>Cannot model intricate inter-feature dependencies</li>
                            <li>Reduces modeling power for multimodal data</li>
                        </ul>

                        <p><strong>5. Scalability Issues:</strong></p>
                        <ul>
                            <li>Computational complexity increases dramatically with state space size</li>
                            <li>Training and inference become prohibitively expensive</li>
                            <li>Limited applicability to high-dimensional problems</li>
                            <li>Requires careful hyperparameter tuning</li>
                        </ul>

                        <p><strong>6. Fixed Structure Complexity:</strong></p>
                        <ul>
                            <li>Must specify number of hidden states in advance</li>
                            <li>No automatic structure learning</li>
                            <li>Difficult to select optimal model complexity</li>
                        </ul>

                        <div class="example-box">
                            <strong>Comparison to Modern Approaches:</strong><br>
                            <table class="comparison-table">
                                <tr>
                                    <td><strong>Aspect</strong></td>
                                    <td><strong>HMM</strong></td>
                                    <td><strong>RNN/LSTM</strong></td>
                                    <td><strong>Transformer</strong></td>
                                </tr>
                                <tr>
                                    <td>Long dependencies</td>
                                    <td>Poor</td>
                                    <td>Better (LSTM)</td>
                                    <td>Excellent</td>
                                </tr>
                                <tr>
                                    <td>Continuous states</td>
                                    <td>No</td>
                                    <td>Yes</td>
                                    <td>Yes</td>
                                </tr>
                                <tr>
                                    <td>Scalability</td>
                                    <td>Limited</td>
                                    <td>Good</td>
                                    <td>Excellent</td>
                                </tr>
                                <tr>
                                    <td>Feature relationships</td>
                                    <td>Limited</td>
                                    <td>Better</td>
                                    <td>Excellent</td>
                                </tr>
                            </table>
                        </div>

                        <p><strong>Solution:</strong> Modern approaches overcome these limitations:</p>
                        <ul>
                            <li><strong>RNNs/LSTMs:</strong> Better handle long dependencies with gating mechanisms</li>
                            <li><strong>Attention mechanisms:</strong> Better capture distant relationships</li>
                            <li><strong>Transformer models:</strong> Parallel processing of entire sequences</li>
                            <li><strong>Neural networks:</strong> Learn more flexible feature representations automatically</li>
                        </ul>
                    </div>
                </div>

                <div class="question-block">
                    <div class="question-title">Q2e: Demonstrate the effect of sparsity penalty on overcomplete autoencoders <span class="marks">5M | CO5</span></div>
                    <div class="answer">
                        <p><strong>Problem Context:</strong> Overcomplete autoencoders have more hidden units than input units, creating risk of learning trivial identity mappings.</p>

                        <div class="part-header">Problem Without Sparsity</div>

                        <div class="emphasis">
                            <strong>Why This Is a Problem:</strong>
                            <ul>
                                <li>Network learns: x ‚Üí x (simply copies input)</li>
                                <li>Hidden layer doesn't compress information</li>
                                <li>No meaningful feature extraction occurs</li>
                                <li>Network becomes useless for dimensionality reduction</li>
                                <li>Cannot discover underlying data structure</li>
                            </ul>
                        </div>

                        <p><strong>Sparsity Penalty Solution:</strong></p>
                        <div class="formula-box">
                            L_total = L_reconstruction + Œª √ó L_sparsity<br>
                            <br>
                            where Œª is the regularization strength parameter<br>
                            and L_sparsity is the sparsity constraint term
                        </div>

                        <div class="part-header">Sparsity Implementation</div>

                        <p><strong>Method 1: KL Divergence Penalty</strong></p>
                        <ul>
                            <li>Enforces target sparsity level (e.g., 5% activation)</li>
                            <li>Measures divergence of actual from target activation</li>
                            <li>Encourages most neurons to remain near zero</li>
                            <li>Only few neurons activate for each input</li>
                        </ul>

                        <p><strong>Method 2: L1 Regularization</strong></p>
                        <ul>
                            <li>Directly penalizes hidden unit magnitudes</li>
                            <li>Encourages weights and activations toward zero</li>
                            <li>Simpler to implement than KL divergence</li>
                            <li>Still creates sparse representations</li>
                        </ul>

                        <div class="part-header">Effects of Sparsity Penalty</div>

                        <table class="comparison-table">
                            <tr>
                                <td><strong>Aspect</strong></td>
                                <td><strong>Without Sparsity</strong></td>
                                <td><strong>With Sparsity</strong></td>
                            </tr>
                            <tr>
                                <td>Hidden activation pattern</td>
                                <td>All neurons active</td>
                                <td>Most inactive, few active</td>
                            </tr>
                            <tr>
                                <td>Learned features</td>
                                <td>Trivial/noisy, identity mapping</td>
                                <td>Meaningful, interpretable features</td>
                            </tr>
                            <tr>
                                <td>Information compression</td>
                                <td>No actual compression</td>
                                <td>Effective compression achieved</td>
                            </tr>
                            <tr>
                                <td>Generalization</td>
                                <td>Poor, overfitting</td>
                                <td>Better, more robust</td>
                            </tr>
                            <tr>
                                <td>Feature interpretability</td>
                                <td>Cannot interpret features</td>
                                <td>Each neuron represents meaningful pattern</td>
                            </tr>
                            <tr>
                                <td>Computational efficiency</td>
                                <td>All neurons active (wasteful)</td>
                                <td>Effective dimensionality reduction</td>
                            </tr>
                        </table>

                        <div class="example-box">
                            <strong>Practical Example:</strong><br>
                            <strong>Setup:</strong> Overcomplete autoencoder with 100 hidden units for 10-input data<br>
                            <br>
                            <strong>Without Sparsity:</strong><br>
                            All 100 units active for every input sample<br>
                            Network doesn't compress information<br>
                            Learns noisy, meaningless features<br>
                            Output: Poor generalization to new data<br>
                            <br>
                            <strong>With Sparsity Penalty:</strong><br>
                            Typically activates only 10-15 units per sample<br>
                            Network learns effective feature compression<br>
                            Each active unit represents interpretable pattern<br>
                            Output: Excellent generalization to new data<br>
                            Result: Discovers 10 latent factors from 100 hidden units
                        </div>

                        <p><strong>Sparsity Coefficient Selection:</strong></p>
                        <ul>
                            <li><strong>Œª too small:</strong> Insufficient regularization, overfitting persists</li>
                            <li><strong>Œª = 0.001-0.01:</strong> Typical range for good results</li>
                            <li><strong>Œª too large:</strong> Over-regularization, network underfits</li>
                            <li><strong>Target sparsity:</strong> Usually 1-10% activation level</li>
                        </ul>

                        <p><strong>Advantages of Sparsity-Regularized Autoencoders:</strong></p>
                        <ul>
                            <li>Discover underlying structure in data</li>
                            <li>Enable dimensionality reduction without sacrificing information</li>
                            <li>Learn interpretable features (each neuron ‚âà one concept)</li>
                            <li>Computational efficiency (few active neurons)</li>
                            <li>Better transfer learning and generalization</li>
                            <li>Biological plausibility (similar to sparse neural coding)</li>
                        </ul>
                    </div>
                </div>
            </div>

            <!-- IA2 PART C -->
            <div class="section" id="ia2-part-c">
                <h2>Internal Assessment - II (IA2) - Part C</h2>
                <p style="color: var(--secondary-color); font-weight: 600;">10 Marks Each | Answer Any 2 out of 3 | Total: 20 Marks | Bloom's Level: L3 (Application/Analysis)</p>

                <div class="question-block">
                    <div class="question-title">Q3a: Sketch the basic architecture of RNN <span class="marks">10M | CO4</span></div>
                    <div class="answer">
                        <p><strong>Definition:</strong> Recurrent Neural Networks (RNN) process sequences by maintaining a hidden state that captures information from all previous inputs, enabling learning of temporal dependencies.</p>

                        <div class="image-container">
                            <img src="https://user-gen-media-assets.s3.amazonaws.com/seedream_images/4ccad23c-1f15-4640-bc51-ed8aae2964d0.png" alt="RNN Architecture">
                            <div class="image-caption">Basic Architecture of Recurrent Neural Network showing temporal connections</div>
                        </div>

                        <div class="part-header">RNN Architecture Components</div>

                        <p><strong>1. Core Components:</strong></p>
                        <ul>
                            <li><strong>Input Layer:</strong> Receives input sequence one element at a time (x_t at time step t)</li>
                            <li><strong>Hidden/Recurrent Layer:</strong> Maintains state h_t across time, processes current input with memory of past</li>
                            <li><strong>Output Layer:</strong> Produces prediction from hidden state (y_t)</li>
                        </ul>

                        <p><strong>2. State Update Mechanism:</strong></p>
                        <div class="formula-box">
                            h_t = tanh(W_hh √ó h_{t-1} + W_xh √ó x_t + b_h)<br>
                            y_t = W_hy √ó h_t + b_y<br>
                            <br>
                            where:<br>
                            - h_t: hidden state at time t<br>
                            - x_t: input at time t<br>
                            - W_hh: weight matrix for hidden-to-hidden connections<br>
                            - W_xh: weight matrix for input-to-hidden connections<br>
                            - W_hy: weight matrix for hidden-to-output<br>
                            - b_h, b_y: bias vectors
                        </div>

                        <p><strong>3. Time Unrolling:</strong></p>
                        <ul>
                            <li>Same weights (W) reused across all time steps</li>
                            <li>Each time step uses previous hidden state</li>
                            <li>Creates computational chain through time</li>
                            <li>Network "unfolds" across time dimension</li>
                        </ul>

                        <p><strong>4. Information Flow:</strong></p>
                        <ol>
                            <li><strong>Forward in time:</strong> x_t ‚Üí hidden layer ‚Üí y_t (current prediction)</li>
                            <li><strong>Recurrent:</strong> h_{t-1} ‚Üí hidden layer ‚Üí h_t (temporal memory)</li>
                            <li><strong>Backward in time:</strong> Gradients flow back through previous states during training</li>
                        </ol>

                        <div class="example-box">
                            <strong>Advantages of RNN Architecture:</strong>
                            <ul>
                                <li><strong>Variable Length Sequences:</strong> Can process sequences of any length</li>
                                <li><strong>Temporal Patterns:</strong> Captures dependencies between elements</li>
                                <li><strong>Parameter Sharing:</strong> Same weights across time reduce parameters</li>
                                <li><strong>Contextual Understanding:</strong> Hidden state acts as memory of past</li>
                                <li><strong>Flexible Applications:</strong> One-to-many, many-to-one, many-to-many mappings</li>
                            </ul>
                        </div>

                        <p><strong>5. RNN Variants & Improvements:</strong></p>
                        <div class="emphasis">
                            <strong>LSTM (Long Short-Term Memory):</strong>
                            <ul>
                                <li>Solves vanishing gradient problem</li>
                                <li>Uses gating mechanisms (input, forget, output gates)</li>
                                <li>Maintains cell state for long-term memory</li>
                                <li>Can learn dependencies over many time steps</li>
                            </ul>
                        </div>

                        <div class="emphasis">
                            <strong>GRU (Gated Recurrent Unit):</strong>
                            <ul>
                                <li>Simpler than LSTM (fewer parameters)</li>
                                <li>Also uses gating to control information flow</li>
                                <li>Combines forget and input gates</li>
                                <li>Good balance between complexity and performance</li>
                            </ul>
                        </div>

                        <p><strong>6. Common Applications:</strong></p>
                        <ul>
                            <li>Machine translation (sequence-to-sequence)</li>
                            <li>Speech recognition (acoustic modeling)</li>
                            <li>Text generation (next character/word prediction)</li>
                            <li>Video action recognition (temporal understanding)</li>
                            <li>Time series forecasting (stock prices, weather)</li>
                            <li>Handwriting recognition</li>
                        </ul>

                        <div class="example-box">
                            <strong>Limitation - Vanishing Gradient Problem:</strong><br>
                            During backpropagation through time (BPTT), gradients multiply through many time steps. If weights < 1, gradients exponentially decay, making early-time learning difficult. This is why LSTM/GRU with gating mechanisms are often preferred.
                        </div>
                    </div>
                </div>

                <div class="question-block">
                    <div class="question-title">Q3b: Illustrate denoising autoencoder <span class="marks">10M | CO5</span></div>
                    <div class="answer">
                        <p><strong>Definition:</strong> A denoising autoencoder is a neural network that learns to reconstruct clean data from corrupted (noisy) inputs, forcing the network to learn robust features that capture the essential structure of data.</p>

                        <div class="image-container">
                            <img src="https://user-gen-media-assets.s3.amazonaws.com/seedream_images/8ea1e093-6f47-4870-b19e-4792d2a7ad1b.png" alt="Denoising Autoencoder">
                            <div class="image-caption">Denoising Autoencoder Architecture showing noise removal process</div>
                        </div>

                        <div class="part-header">Architecture and Components</div>

                        <p><strong>1. Input Corruption Stage:</strong></p>
                        <ul>
                            <li><strong>Original data:</strong> Clean input x</li>
                            <li><strong>Corruption:</strong> xÃÉ = x + noise</li>
                            <li><strong>Noise types:</strong>
                                <ul>
                                    <li>Gaussian noise (continuous random perturbation)</li>
                                    <li>Salt-and-pepper noise (random pixel flipping)</li>
                                    <li>Random masking (randomly zeroing inputs)</li>
                                    <li>Dropout noise (stochastic zeroing)</li>
                                </ul>
                            </li>
                            <li><strong>Corruption ratio:</strong> Typically 10-50% of input</li>
                        </ul>

                        <p><strong>2. Encoder Component:</strong></p>
                        <ul>
                            <li>Processes noisy input xÃÉ (not clean input)</li>
                            <li>Compresses to latent representation z</li>
                            <li>Formula: z = Encoder(xÃÉ) = œÉ(W_enc √ó xÃÉ + b_enc)</li>
                            <li>Dimensionality reduction creates bottleneck</li>
                        </ul>

                        <p><strong>3. Bottleneck Layer:</strong></p>
                        <ul>
                            <li>Acts as information filter</li>
                            <li>Constraint forces information compression</li>
                            <li>Noise is NOT preserved (too sparse to fit)</li>
                            <li>Creates clean latent representation</li>
                            <li>Typically has dimensions < input dimensions</li>
                        </ul>

                        <p><strong>4. Decoder Component:</strong></p>
                        <ul>
                            <li>Reconstructs output from latent representation z</li>
                            <li>Formula: xÃÇ = Decoder(z) = œÉ(W_dec √ó z + b_dec)</li>
                            <li>Aims to recover clean original data x (not noisy xÃÉ)</li>
                            <li>Output dimensions equal original input dimensions</li>
                        </ul>

                        <p><strong>5. Training Objective:</strong></p>
                        <div class="formula-box">
                            Loss function: L = ||x - xÃÇ||¬≤<br>
                            <br>
                            Important: Loss compares output to ORIGINAL (clean) data,<br>
                            not the corrupted input!<br>
                            <br>
                            This forces encoder to separate signal from noise.
                        </div>

                        <div class="part-header">Why Denoising Works</div>

                        <div class="example-box">
                            <strong>Key Insight:</strong><br>
                            Network cannot simply copy corrupted input (bottleneck prevents this)<br>
                            ‚Üì<br>
                            Must learn essential features for reconstruction<br>
                            ‚Üì<br>
                            Noise is discarded as non-informative<br>
                            ‚Üì<br>
                            Results in more robust representations
                        </div>

                        <p><strong>Learning Process:</strong></p>
                        <ol>
                            <li>Noisy input enters encoder</li>
                            <li>Bottleneck filter removes noise</li>
                            <li>Decoder reconstructs clean output</li>
                            <li>Loss compares to original (clean) target</li>
                            <li>Backpropagation encourages signal preservation, noise removal</li>
                        </ol>

                        <div class="part-header">Advantages and Applications</div>

                        <p><strong>Advantages:</strong></p>
                        <ul>
                            <li><strong>Robustness:</strong> Learned representations robust to noise</li>
                            <li><strong>Unsupervised:</strong> No labels needed, only data and corrupted versions</li>
                            <li><strong>Feature Learning:</strong> Automatically discovers denoising features</li>
                            <li><strong>Generalization:</strong> Improved transfer to downstream tasks</li>
                            <li><strong>Noise Handling:</strong> Can denoise multiple noise types</li>
                        </ul>

                        <p><strong>Applications:</strong></p>
                        <ul>
                            <li><strong>Image Denoising:</strong> Medical imaging (MRI, CT scans), old photographs, satellite imagery</li>
                            <li><strong>Feature Extraction:</strong> Learning good features from noisy data</li>
                            <li><strong>Adversarial Robustness:</strong> Improving robustness to adversarial examples</li>
                            <li><strong>Data Preprocessing:</strong> Cleaning data before analysis</li>
                            <li><strong>Representation Learning:</strong> Discovering interpretable factors of variation</li>
                        </ul>

                        <p><strong>Noise Types and Effects:</strong></p>
                        <table class="comparison-table">
                            <tr>
                                <td><strong>Noise Type</strong></td>
                                <td><strong>Description</strong></td>
                                <td><strong>Use Case</strong></td>
                            </tr>
                            <tr>
                                <td>Gaussian Noise</td>
                                <td>Continuous random perturbation</td>
                                <td>Natural sensor noise, real-world observations</td>
                            </tr>
                            <tr>
                                <td>Salt-and-pepper</td>
                                <td>Random pixel flipping (0-255)</td>
                                <td>Image corruption, digital noise</td>
                            </tr>
                            <tr>
                                <td>Masking Noise</td>
                                <td>Random values set to zero</td>
                                <td>Missing data, occlusion</td>
                            </tr>
                            <tr>
                                <td>Dropout-based</td>
                                <td>Stochastic neuron deactivation</td>
                                <td>Feature dropout, regularization</td>
                            </tr>
                        </table>

                        <p><strong>Comparison with Standard Autoencoder:</strong></p>
                        <table class="comparison-table">
                            <tr>
                                <td><strong>Aspect</strong></td>
                                <td><strong>Standard Autoencoder</strong></td>
                                <td><strong>Denoising Autoencoder</strong></td>
                            </tr>
                            <tr>
                                <td>Input</td>
                                <td>Clean data x</td>
                                <td>Corrupted data xÃÉ</td>
                            </tr>
                            <tr>
                                <td>Target</td>
                                <td>Reconstruct x</td>
                                <td>Reconstruct original x</td>
                            </tr>
                            <tr>
                                <td>Purpose</td>
                                <td>Dimensionality reduction</td>
                                <td>Robust feature learning</td>
                            </tr>
                            <tr>
                                <td>Feature quality</td>
                                <td>May overfit to noise</td>
                                <td>Robust to noise variations</td>
                            </tr>
                            <tr>
                                <td>Generalization</td>
                                <td>Moderate</td>
                                <td>Better</td>
                            </tr>
                        </table>
                    </div>
                </div>

                <div class="question-block">
                    <div class="question-title">Q3c: Demonstrate 2D convolution operation in CNN <span class="marks">10M | CO6</span></div>
                    <div class="answer">
                        <p><strong>Fundamental Concept:</strong> 2D convolution applies a small learnable filter (kernel) across an image to extract local features, forming the foundation of Convolutional Neural Networks.</p>

                        <div class="image-container">
                            <img src="https://user-gen-media-assets.s3.amazonaws.com/seedream_images/3f46d4be-c59a-47c7-a668-036bdb6421da.png" alt="2D Convolution Operation">
                            <div class="image-caption">2D Convolution Operation in CNN with kernel sliding and feature extraction</div>
                        </div>

                        <div class="part-header">Mathematical Foundation</div>

                        <p><strong>Convolution Formula:</strong></p>
                        <div class="formula-box">
                            Output(i, j) = Œ£ Œ£ Kernel(m, n) √ó Input(i+m, j+n) + bias<br>
                            <br>
                            where summation is over kernel dimensions
                        </div>

                        <div class="part-header">Step-by-Step Example</div>

                        <p><strong>Input Image (5√ó5):</strong></p>
                        <div class="formula-box">
                            [1  0  2  3  1]<br>
                            [0  1  1  2  0]<br>
                            [2  1  0  1  3]<br>
                            [1  2  1  0  2]<br>
                            [0  1  2  1  1]
                        </div>

                        <p><strong>Kernel/Filter (3√ó3) - Edge Detection:</strong></p>
                        <div class="formula-box">
                            [ 1  0 -1]<br>
                            [ 0  1  0]<br>
                            [-1  0  1]
                        </div>

                        <p><strong>Convolution at Position (0,0):</strong></p>
                        <div class="formula-box">
                            Apply kernel to top-left 3√ó3 region:<br>
                            [1  0  2]     [1  0 -1]<br>
                            [0  1  1]  √ó  [0  1  0]<br>
                            [2  1  0]     [-1 0  1]<br>
                            <br>
                            Calculation:<br>
                            = 1√ó1 + 0√ó0 + 2√ó(-1)<br>
                            &nbsp;+ 0√ó0 + 1√ó1 + 1√ó0<br>
                            &nbsp;+ 2√ó(-1) + 1√ó0 + 0√ó1<br>
                            = 1 + 0 - 2 + 0 + 1 + 0 - 2 + 0 + 0<br>
                            = -2
                        </div>

                        <p>Output value at (0,0) = -2</p>

                        <p><strong>Repeat for all valid positions</strong> with sliding window to produce complete output feature map.</p>

                        <div class="part-header">Key Convolution Parameters</div>

                        <p><strong>1. Stride (Step Size):</strong></p>
                        <ul>
                            <li><strong>Stride = 1:</strong> Move kernel one pixel at a time (maximum coverage)</li>
                            <li><strong>Stride = 2:</strong> Move kernel two pixels (faster, reduced output size)</li>
                            <li><strong>Higher stride:</strong> Fewer output elements, faster computation, but fewer features</li>
                        </ul>

                        <p><strong>2. Padding:</strong></p>
                        <ul>
                            <li><strong>No padding:</strong> Output smaller than input</li>
                            <li><strong>Same padding:</strong> Add zeros to maintain dimensions</li>
                            <li><strong>Valid padding:</strong> No padding, only valid regions</li>
                            <li><strong>Purpose:</strong> Preserve spatial information, especially at borders</li>
                        </ul>

                        <p><strong>3. Multiple Filters:</strong></p>
                        <ul>
                            <li>Each filter learns different features</li>
                            <li><strong>First layer filters:</strong> Edges, corners, simple patterns</li>
                            <li><strong>Middle layer filters:</strong> Shapes, textures, object parts</li>
                            <li><strong>Deep layer filters:</strong> Objects, scenes, high-level concepts</li>
                        </ul>

                        <div class="part-header">Output Dimension Calculation</div>

                        <p><strong>Formula:</strong></p>
                        <div class="formula-box">
                            Output_height = (Input_height - Kernel_height + 2√óPadding) / Stride + 1<br>
                            Output_width = (Input_width - Kernel_width + 2√óPadding) / Stride + 1
                        </div>

                        <p><strong>Example:</strong></p>
                        <ul>
                            <li>Input: 32√ó32, Kernel: 3√ó3, Stride: 1, Padding: 1</li>
                            <li>Output: (32 - 3 + 2) / 1 + 1 = 32√ó32</li>
                            <li>Result: Same size output (good for deep networks)</li>
                        </ul>

                        <div class="part-header">Feature Hierarchy in CNNs</div>

                        <p><strong>Progressive Feature Learning:</strong></p>
                        <ol>
                            <li><strong>Layer 1:</strong> Low-level features
                                <ul>
                                    <li>Horizontal edges</li>
                                    <li>Vertical edges</li>
                                    <li>Diagonal patterns</li>
                                    <li>Color blobs</li>
                                </ul>
                            </li>
                            <li><strong>Layer 2-3:</strong> Mid-level features
                                <ul>
                                    <li>Corners and junctions</li>
                                    <li>Circular shapes</li>
                                    <li>Textures (grass, wood)</li>
                                    <li>Simple object parts</li>
                                </ul>
                            </li>
                            <li><strong>Layer 4-5:</strong> High-level features
                                <ul>
                                    <li>Object parts (wheels, ears)</li>
                                    <li>Object semantics (dog, cat)</li>
                                    <li>Scene understanding (outdoor, indoor)</li>
                                    <li>Complex patterns and structures</li>
                                </ul>
                            </li>
                        </ol>

                        <div class="example-box">
                            <strong>Real-World Impact:</strong><br>
                            Early convolution filters learn edge detection ‚Üí Progressive abstraction ‚Üí Final layers understand semantics<br>
                            <br>
                            Result: State-of-the-art performance in:<br>
                            ‚Ä¢ Image classification (ImageNet accuracy > 99%)<br>
                            ‚Ä¢ Object detection (YOLO, Faster R-CNN)<br>
                            ‚Ä¢ Semantic segmentation (pixel-level classification)<br>
                            ‚Ä¢ Medical image analysis (disease diagnosis)
                        </div>

                        <div class="part-header">Advantages of Convolution</div>

                        <p><strong>Compared to Fully Connected Layers:</strong></p>
                        <table class="comparison-table">
                            <tr>
                                <td><strong>Property</strong></td>
                                <td><strong>Fully Connected</strong></td>
                                <td><strong>Convolution</strong></td>
                            </tr>
                            <tr>
                                <td>Parameters</td>
                                <td>Very large (full connectivity)</td>
                                <td>Small (weight sharing)</td>
                            </tr>
                            <tr>
                                <td>Local connectivity</td>
                                <td>Global (all to all)</td>
                                <td>Local (receptive field)</td>
                            </tr>
                            <tr>
                                <td>Spatial structure</td>
                                <td>Ignored</td>
                                <td>Preserved and exploited</td>
                            </tr>
                            <tr>
                                <td>Interpretability</td>
                                <td>Black box</td>
                                <td>Visualizable filters</td>
                            </tr>
                            <tr>
                                <td>Computational efficiency</td>
                                <td>Expensive</td>
                                <td>Fast (highly optimized)</td>
                            </tr>
                        </table>

                        <p><strong>Key Insights:</strong></p>
                        <ul>
                            <li><strong>Weight Sharing:</strong> Same kernel reused across image ‚Üí fewer parameters than fully connected</li>
                            <li><strong>Local Processing:</strong> Neurons only look at local regions ‚Üí biological plausibility</li>
                            <li><strong>Hierarchical Learning:</strong> Stacking convolutions builds progressively complex features</li>
                            <li><strong>Translation Invariance:</strong> Learned features work regardless of position in image</li>
                            <li><strong>Efficiency:</strong> Convolution highly optimized on GPUs (CUDA kernels)</li>
                        </ul>
                    </div>
                </div>
            </div>

            <!-- SUMMARY SECTION -->
            <div class="section" id="summary">
                <h2>üìä Summary & Quick Reference</h2>

                <h3>Quick Stats</h3>
                <div class="progress-bar">
                    <div class="progress-fill" style="width: 100%; background: linear-gradient(90deg, #2d5f7f, #4a90a4);">
                        26 Total Questions Solved
                    </div>
                </div>

                <div class="summary-grid">
                    <div class="summary-card">
                        <h4>üìã IA1</h4>
                        <p><strong>13 Questions</strong><br>Total: 50 Marks<br>Part A: 5√ó2M<br>Part B: 5√ó5M<br>Part C: 3√ó10M</p>
                    </div>
                    <div class="summary-card">
                        <h4>üìã IA2</h4>
                        <p><strong>13 Questions</strong><br>Total: 50 Marks<br>Part A: 5√ó2M<br>Part B: 5√ó5M<br>Part C: 3√ó10M</p>
                    </div>
                    <div class="summary-card">
                        <h4>üéØ Topics Covered</h4>
                        <p><strong>6 Core Areas</strong><br>Perceptrons & Neural Networks<br>RNN & LSTM<br>Autoencoders<br>CNN & Convolution<br>Backpropagation<br>HMM Limitations</p>
                    </div>
                </div>

                <h3>üìå Important Concepts Quick Reference</h3>

                <table>
                    <thead>
                        <tr>
                            <th>Concept</th>
                            <th>Key Definition</th>
                            <th>Application</th>
                            <th>Related CO</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>Pattern</strong></td>
                            <td>Recurring structure in data</td>
                            <td>Classification, recognition</td>
                            <td>CO1</td>
                        </tr>
                        <tr>
                            <td><strong>Perceptron</strong></td>
                            <td>Single neuron classifier</td>
                            <td>Binary classification</td>
                            <td>CO1</td>
                        </tr>
                        <tr>
                            <td><strong>Multi-Layer Perceptron</strong></td>
                            <td>Multiple layers with non-linearity</td>
                            <td>Complex classification</td>
                            <td>CO2</td>
                        </tr>
                        <tr>
                            <td><strong>XOR Problem</strong></td>
                            <td>Linearly non-separable problem</td>
                            <td>Benchmark for testing networks</td>
                            <td>CO2</td>
                        </tr>
                        <tr>
                            <td><strong>Backpropagation</strong></td>
                            <td>Gradient computation algorithm</td>
                            <td>Training neural networks</td>
                            <td>CO2</td>
                        </tr>
                        <tr>
                            <td><strong>RNN</strong></td>
                            <td>Sequential data with memory</td>
                            <td>Time series, NLP</td>
                            <td>CO4</td>
                        </tr>
                        <tr>
                            <td><strong>Autoencoder</strong></td>
                            <td>Unsupervised feature learning</td>
                            <td>Dimensionality reduction</td>
                            <td>CO5</td>
                        </tr>
                        <tr>
                            <td><strong>CNN</strong></td>
                            <td>Convolution-based network</td>
                            <td>Image processing</td>
                            <td>CO6</td>
                        </tr>
                    </tbody>
                </table>

                <h3>üéì Study Tips for Exam</h3>
                <div class="emphasis">
                    <strong>Part A Questions (2 marks):</strong>
                    <ul>
                        <li>Provide concise, accurate definitions</li>
                        <li>Include one practical example</li>
                        <li>Focus on key characteristics</li>
                        <li>Time: ~3-4 minutes per question</li>
                    </ul>
                </div>

                <div class="emphasis">
                    <strong>Part B Questions (5 marks):</strong>
                    <ul>
                        <li>Include diagrams/sketches where helpful</li>
                        <li>Explain architecture or process flow</li>
                        <li>Provide numerical examples</li>
                        <li>Connect theory to applications</li>
                        <li>Time: ~7-8 minutes per question</li>
                    </ul>
                </div>

                <div class="emphasis">
                    <strong>Part C Questions (10 marks):</strong>
                    <ul>
                        <li>Comprehensive treatment with all steps</li>
                        <li>Include mathematical formulations</li>
                        <li>Explain algorithms step-by-step</li>
                        <li>Provide detailed examples</li>
                        <li>Discuss advantages and limitations</li>
                        <li>Time: ~15-18 minutes per question</li>
                    </ul>
                </div>

                <h3>üîó Key Formulas Reference</h3>
                <div class="formula-box">
                    <strong>Perceptron Update:</strong> w = w + Œ∑(y - ≈∑)x<br>
                    <strong>RNN State:</strong> h_t = tanh(W_hh h_{t-1} + W_xh x_t)<br>
                    <strong>Backpropagation Error:</strong> Œ¥^(l) = (W^(l+1)·µÄ Œ¥^(l+1)) ‚äô œÉ'(z^(l))<br>
                    <strong>Convolution:</strong> Output = Œ£ Kernel ‚äô Input + bias<br>
                    <strong>Autoencoder Loss:</strong> L = ||x - Decoder(Encoder(x))||¬≤
                </div>

                <h3>üìö Important Visual Concepts</h3>
                <p>This guide includes visual illustrations for:</p>
                <ul>
                    <li>‚úÖ RNN Architecture with recurrent connections</li>
                    <li>‚úÖ Denoising Autoencoder noise removal process</li>
                    <li>‚úÖ 2D Convolution kernel sliding operation</li>
                    <li>‚úÖ Backpropagation forward and backward passes</li>
                    <li>‚úÖ XOR Problem solution with multi-layer network</li>
                    <li>‚úÖ Standard Autoencoder compression</li>
                    <li>‚úÖ Linear Dynamical Systems state evolution</li>
                </ul>

                <h3>‚úÖ Exam Preparation Checklist</h3>
                <div class="emphasis">
                    <input type="checkbox"> Understand all course outcomes (CO1-CO6)<br>
                    <input type="checkbox"> Review all 26 questions and sample answers<br>
                    <input type="checkbox"> Study visual diagrams for better understanding<br>
                    <input type="checkbox"> Practice writing concise definitions (Part A)<br>
                    <input type="checkbox"> Practice drawing architecture diagrams (Part B)<br>
                    <input type="checkbox"> Practice detailed explanations (Part C)<br>
                    <input type="checkbox"> Review formulas and mathematical concepts<br>
                    <input type="checkbox"> Understand real-world applications<br>
                    <input type="checkbox"> Practice time management (90 minutes)<br>
                    <input type="checkbox"> Review limitations and comparisons<br>
                </div>

                <div class="footer">
                    <p><strong>Study Guide for 22CSE157 - Fundamentals of Deep Learning</strong></p>
                    <p>Complete Solutions for Internal Assessments I & II</p>
                    <p>Generated with detailed explanations, examples, and visual illustrations</p>
                    <p style="margin-top: 15px; font-size: 0.9em;">üí° Tip: Use this as a comprehensive reference guide and supplement with your course materials for best results.</p>
                </div>
            </div>
        </div>
    </div>

    <script>
        // Navigation functionality
        const navButtons = document.querySelectorAll('.nav-btn');
        const sections = document.querySelectorAll('.section');

        navButtons.forEach(button => {
            button.addEventListener('click', () => {
                // Remove active class from all buttons and sections
                navButtons.forEach(btn => btn.classList.remove('active'));
                sections.forEach(section => section.classList.remove('active'));

                // Add active class to clicked button and corresponding section
                button.classList.add('active');
                const sectionId = button.getAttribute('data-section');
                document.getElementById(sectionId).classList.add('active');
            });
        });

        // Search functionality
        const searchBox = document.getElementById('searchBox');
        searchBox.addEventListener('input', (e) => {
            const searchTerm = e.target.value.toLowerCase();
            const activeSection = document.querySelector('.section.active');
            const questionBlocks = activeSection.querySelectorAll('.question-block');
            
            if (!searchTerm) {
                questionBlocks.forEach(block => block.style.display = 'block');
                return;
            }

            let foundCount = 0;
            questionBlocks.forEach(block => {
                const title = block.querySelector('.question-title').textContent.toLowerCase();
                const answer = block.querySelector('.answer').textContent.toLowerCase();
                
                if (title.includes(searchTerm) || answer.includes(searchTerm)) {
                    block.style.display = 'block';
                    foundCount++;
                } else {
                    block.style.display = 'none';
                }
            });

            if (foundCount === 0 && searchTerm) {
                const noResults = document.createElement('div');
                noResults.className = 'no-results';
                noResults.textContent = `No results found for "${searchTerm}" in this section`;
            }
        });

        // Highlight active section background
        document.addEventListener('DOMContentLoaded', () => {
            // Set overview as default
            const overviewBtn = document.querySelector('[data-section="overview"]');
            if (overviewBtn) {
                overviewBtn.click();
            }
        });
    </script>
</body>
</html>
